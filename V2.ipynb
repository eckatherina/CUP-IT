{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6887,
     "status": "ok",
     "timestamp": 1678895686292,
     "user": {
      "displayName": "Денис Фауч",
      "userId": "17825234106867911063"
     },
     "user_tz": -600
    },
    "id": "hcb36j8nupfl",
    "outputId": "70e6c420-703c-4cac-98e3-3d0bf25dfc68"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1678895686294,
     "user": {
      "displayName": "Денис Фауч",
      "userId": "17825234106867911063"
     },
     "user_tz": -600
    },
    "id": "it70VP5mvq2n",
    "outputId": "9a020055-e145-4107-e2b2-66b12ff34480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/СL_CuP\n"
     ]
    }
   ],
   "source": [
    "%cd /content/gdrive/MyDrive/СL_CuP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51882,
     "status": "ok",
     "timestamp": 1678895738169,
     "user": {
      "displayName": "Денис Фауч",
      "userId": "17825234106867911063"
     },
     "user_tz": -600
    },
    "id": "_esfIAZ2wqez",
    "outputId": "74bbefd9-d919-4166-bdff-1d9062e55b5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 440535\n",
      "Test samples: 70020\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 440535 entries, 0 to 440534\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   text     440535 non-null  object\n",
      " 1   comment  440535 non-null  object\n",
      " 2   score    440535 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 10.1+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70020 entries, 0 to 70019\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     70020 non-null  object\n",
      " 1   comment  70020 non-null  object\n",
      " 2   score    0 non-null      object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "               score\n",
      "count  440535.000000\n",
      "mean        2.000000\n",
      "std         1.414215\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         2.000000\n",
      "75%         3.000000\n",
      "max         4.000000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "    \n",
    "with open('ranking_train.jsonl', 'r') as f:\n",
    "    train_data = list(f)\n",
    "\n",
    "with open('ranking_test.jsonl', 'r') as f:\n",
    "    test_data = list(f)\n",
    "    \n",
    "def parse_json_line(json_line):\n",
    "    json_data = json.loads(json_line)\n",
    "    text = json_data['text']\n",
    "    comments = json_data['comments']\n",
    "    comments_text = [comment['text'] for comment in comments]\n",
    "    comments_score = [comment['score'] for comment in comments]\n",
    "    return pd.DataFrame({'text': [text]*5, 'comment': comments_text, 'score': comments_score})\n",
    "\n",
    "train_df = pd.concat([parse_json_line(line) for line in train_data], ignore_index=True)\n",
    "test_df = pd.concat([parse_json_line(line) for line in test_data], ignore_index=True)\n",
    "# Размер обучающей и тестовой выборок\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Общая информация по датафреймам\n",
    "print(train_df.info())\n",
    "print(test_df.info())\n",
    "\n",
    "# Статистические характеристики данных\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1678895738170,
     "user": {
      "displayName": "Денис Фауч",
      "userId": "17825234106867911063"
     },
     "user_tz": -600
    },
    "id": "IFG5MiBUxDs0",
    "outputId": "0cdd1399-be3c-43d2-c88b-d01be66ad21d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Гистограмма распределения оценок комментариев\n",
    "sns.countplot(x='score', data=train_df)\n",
    "plt.title('Score distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1678895738171,
     "user": {
      "displayName": "Денис Фауч",
      "userId": "17825234106867911063"
     },
     "user_tz": -600
    },
    "id": "SAiQd-TBxLUZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def install_java():\n",
    "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "  !java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 52718,
     "status": "ok",
     "timestamp": 1678895790876,
     "user": {
      "displayName": "Денис Фауч",
      "userId": "17825234106867911063"
     },
     "user_tz": -600
    },
    "id": "L7Xi7nprxM0e"
   },
   "outputs": [],
   "source": [
    "%%capture installation.log\n",
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "install_java()\n",
    "!pip install spacy\n",
    "!pip install language_tool_python\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install psutil\n",
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
      "     ------------------------------------ 100.6/100.6 kB 444.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-hub) (1.21.5)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-hub) (3.19.6)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1678895790877,
     "user": {
      "displayName": "Денис Фауч",
      "userId": "17825234106867911063"
     },
     "user_tz": -600
    },
    "id": "ojyihh9-KW1v",
    "outputId": "b3142684-37ea-4f65-b0d5-19d9b578404a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "4231888896\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(psutil.cpu_count(logical=True))\n",
    "\n",
    "# Возвращает объект, содержащий информацию о памяти\n",
    "memory = psutil.virtual_memory()\n",
    "\n",
    "# Выводит доступную память в байтах\n",
    "print(memory.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25531"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# освобождает неиспользуемые объекты из памяти\n",
    "# del some_object\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4230553600\n"
     ]
    }
   ],
   "source": [
    "memory = psutil.virtual_memory()\n",
    "\n",
    "# Выводит доступную память в байтах\n",
    "print(memory.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting memory_profiler\n",
      "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\lib\\site-packages (from memory_profiler) (5.9.0)\n",
      "Installing collected packages: memory_profiler\n",
      "Successfully installed memory_profiler-0.61.0\n"
     ]
    }
   ],
   "source": [
    "!pip install memory_profiler\n",
    "from memory_profiler import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28454,
     "status": "ok",
     "timestamp": 1678896098617,
     "user": {
      "displayName": "Денис Фауч",
      "userId": "17825234106867911063"
     },
     "user_tz": -600
    },
    "id": "5F_YHooqJc0H",
    "outputId": "8078cc96-066f-4bf7-a73d-979357c15a56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%%capture analyze_comments.log\n",
    "import re\n",
    "import csv\n",
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import nltk\n",
    "import multiprocessing as mp\n",
    "import language_tool_python\n",
    "import concurrent.futures as futures\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel, AutoTokenizer, TextClassificationPipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "\n",
    "# Load pre-trained models\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# Initialize tokenizer and pipeline\n",
    "model_path = \"martin-ha/toxic-comment-model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "tokenizer_d = AutoTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "model_d = AutoModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_toxicity(comment, max_length=512, stride=256):\n",
    "\n",
    "#     # Split comment into smaller segments\n",
    "#     segments = []\n",
    "#     if len(comment) > max_length:\n",
    "#         # Split comment into overlapping segments\n",
    "#         for i in range(0, len(comment), stride):\n",
    "#             segment = comment[i:i+max_length]\n",
    "#             # Force segment to be exactly 512 characters\n",
    "#             segment = segment[:512]\n",
    "#             segments.append(segment)\n",
    "#     else:\n",
    "#         segments.append(comment)\n",
    "\n",
    "#     # Calculate toxicity score for each segment\n",
    "#     toxicity_scores = []\n",
    "#     for segment in segments:\n",
    "#         input_ids = tokenizer.encode(segment, truncation=True, padding=True, return_tensors='pt')\n",
    "#         tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "#         text = tokenizer.convert_tokens_to_string(tokens)\n",
    "#         output = pipeline(text)\n",
    "#         toxicity_scores.append(output[0]['score'])\n",
    "    \n",
    "#     # Average toxicity scores across segments\n",
    "#     toxicity_score = np.mean(toxicity_scores)\n",
    "\n",
    "#     return toxicity_score\n",
    "\n",
    "# def compute_use_vectors(text, model, tokenizer, max_length=512, stride=256):\n",
    "#     # Split input sequence into smaller segments\n",
    "#     segments = []\n",
    "#     if len(text) > max_length:\n",
    "#         # Split text into overlapping segments\n",
    "#         for i in range(0, len(text), stride):\n",
    "#             segment = text[i:i+max_length]\n",
    "#             # Force segment to be exactly 512 characters\n",
    "#             segment = segment[:512]\n",
    "#             segments.append(segment)\n",
    "#     else:\n",
    "#         segments.append(text)\n",
    "\n",
    "#     # Compute embeddings for each segment\n",
    "#     batch_size = 8\n",
    "#     embeddings = []\n",
    "#     for i in range(0, len(segments), batch_size):\n",
    "#         batch_segments = segments[i:i+batch_size]\n",
    "#         batch_input = tokenizer.batch_encode_plus(batch_segments, padding = True, truncation=True, return_tensors='pt')\n",
    "#         batch_embeddings = model(batch_input['input_ids'], batch_input['attention_mask'])\n",
    "#         last_hidden_state_tensor = batch_embeddings.last_hidden_state\n",
    "#         embeddings.append(last_hidden_state_tensor.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "#     # Concatenate embeddings to obtain full vector representation, even if embeddings is empty\n",
    "#     if len(embeddings) > 0:\n",
    "#         embeddings = np.concatenate(embeddings, axis=0)\n",
    "#     else:\n",
    "#         embeddings = np.array([])\n",
    "\n",
    "#     return torch.tensor(np.stack(embeddings))\n",
    "\n",
    "@profile\n",
    "@jit(nopython=True)\n",
    "def analyze_comment(comment, post_text, score, sia):\n",
    "    # Find number of words\n",
    "    words = comment.split()\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Find number of unique words\n",
    "    unique_words = set(words)\n",
    "    num_unique_words = len(unique_words)\n",
    "    \n",
    "    # Find number of letters and punctuation marks\n",
    "    num_letters = sum([len(word) for word in words])\n",
    "    num_punctuation = sum([1 for char in comment if char in string.punctuation])\n",
    "    \n",
    "    # Find number of uppercase letters and words\n",
    "    num_uppercase_letters = sum([1 for char in comment if char.isupper()])\n",
    "    num_uppercase_words = sum([1 for word in words if word.isupper()])\n",
    "    \n",
    "    # Find number of stop words\n",
    "    # num_stop_words = len([word for word in nlp(comment) if word.is_stop])\n",
    "    num_stop_words = sum([word.is_stop for word in nlp(comment)])\n",
    "    \n",
    "    # Find average word length\n",
    "    if num_words > 0:\n",
    "        avg_word_length = num_letters / num_words\n",
    "    else:\n",
    "        avg_word_length = 0\n",
    "    \n",
    "    # Find number of sentences\n",
    "    # sentences = list(nlp(comment).sents)\n",
    "    # num_sentences = len(sentences)\n",
    "    num_sentences = sum(1 for _ in nlp(comment).sents)\n",
    "    \n",
    "    # Calculate sentiment score\n",
    "    sentiment_score = sia.polarity_scores(comment)['compound']\n",
    "    \n",
    "    # Compute embeddings for post and comment\n",
    "#     post_embedding = compute_use_vectors(post_text, model = model_d, tokenizer = tokenizer_d)[0]\n",
    "#     comment_embedding = compute_use_vectors(comment, model = model_d, tokenizer = tokenizer_d)[0]\n",
    "    \n",
    "    # # pad the shorter vector with zeros\n",
    "    # if comment_embedding.shape[1] > post_embedding.shape[1]:\n",
    "    #     pad_width = ((0, 0), (0, comment_embedding.shape[1]-post_embedding.shape[1]))\n",
    "    #     post_embedding = np.pad(post_embedding, pad_width, mode='constant')\n",
    "    # else:\n",
    "    #     pad_width = ((0, 0), (0, post_embedding.shape[1]-comment_embedding.shape[1]))\n",
    "    #     comment_embedding = np.pad(comment_embedding, pad_width, mode='constant')\n",
    "\n",
    "    # # compute cosine similarity\n",
    "    # similarity_score = cosine_similarity(comment_embedding, post_embedding)[0][0]\n",
    "\n",
    "    # Calculate toxicity score\n",
    "    # print(type(comment))\n",
    "    toxicity_score = evaluate_toxicity(comment, max_length=512, stride=256)\n",
    "    \n",
    "    return (post_text,\n",
    "            comment,\n",
    "            score,\n",
    "            num_words,\n",
    "            num_unique_words,\n",
    "            avg_word_length,\n",
    "            num_punctuation,\n",
    "            num_uppercase_letters,\n",
    "            num_uppercase_words,\n",
    "            num_stop_words,\n",
    "            num_sentences,\n",
    "            len(re.findall(r'\\d+', comment)), \n",
    "            len(re.findall(r'http\\S+', comment)), \n",
    "            len(re.findall(r'#\\w+', comment)), \n",
    "            len(re.findall(r'[^\\w\\s,]', comment)), \n",
    "            len(re.findall(r'\\b[A-Z]{2,}\\b', comment)), \n",
    "            len(tool.check(comment)),\n",
    "            comment.lower().count('Edit'), \n",
    "            sum([1 for word in comment.split() if not word.isascii()]), \n",
    "            num_punctuation / len(comment), \n",
    "            len([ent for ent in nlp(comment).ents if ent.label_ in ['PERSON', 'ORG', 'GPE']]), \n",
    "            sentiment_score, \n",
    "            # similarity_score, \n",
    "            toxicity_score)\n",
    "\n",
    "\n",
    "# def print_feature_statistics(result):\n",
    "#     feature_names = list(result.keys())\n",
    "#     feature_frequencies = [len(result[name]) for name in feature_names]\n",
    "#     feature_ranks = rankdata(feature_frequencies, method='dense')\n",
    "#     feature_rank_dict = {name: rank for name, rank in zip(feature_names, feature_ranks)}\n",
    "\n",
    "#     # Sort features by their ranks\n",
    "#     sorted_features = sorted(feature_rank_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "#     # Print results\n",
    "#     for feature, rank in sorted_features:\n",
    "#         values = result[feature]\n",
    "#         if isinstance(values[0], np.ndarray):\n",
    "#             values = [list(v) for v in values]\n",
    "#         print(f\"{feature} (rank {rank}):\")\n",
    "#         print(f\"\\tMin: {np.min(values)}\")\n",
    "#         print(f\"\\tMax: {np.max(values)}\")\n",
    "#         print(f\"\\tMean: {np.mean(values)}\")\n",
    "#         print(f\"\\tMedian: {np.median(values)}\")\n",
    "#         print(f\"\\tStd: {np.std(values)}\")\n",
    "#         print(\"\")\n",
    "        \n",
    "# def convert_to_csv(result, filename):\n",
    "#     feature_names = list(result.keys())\n",
    "#     feature_frequencies = [len(result[name]) for name in feature_names]\n",
    "#     feature_ranks = rankdata(feature_frequencies, method='dense')\n",
    "#     feature_rank_dict = {name: rank for name, rank in zip(feature_names, feature_ranks)}\n",
    "\n",
    "#     # Sort features by their ranks\n",
    "#     sorted_features = sorted(feature_rank_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "#     with open(filename, 'w', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(['Feature', 'Rank', 'Min', 'Max', 'Mean', 'Median', 'Std'])\n",
    "#         for feature, rank in sorted_features:\n",
    "#             values = result[feature]\n",
    "#             if isinstance(values[0], np.ndarray):\n",
    "#                 values = [list(v) for v in values]\n",
    "#             writer.writerow([feature, rank, np.min(values), np.max(values), np.mean(values), np.median(values), np.std(values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def analyze_comments(df, sia):\n",
    "\n",
    "    # Create a partial function to compute USE vectors\n",
    "    # compute_use_vectors_partial = partial(compute_use_vectors, use=use)\n",
    "\n",
    "#     with ProcessPoolExecutor() as executor:\n",
    "#         # Analyze comments in parallel\n",
    "#         futures = [executor.submit(analyze_comment, row.comment, row.text, row.score) for row in df.itertuples()]\n",
    "\n",
    "#         # Wait for jobs to complete and process results\n",
    "#         for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "#             results.append(future.result())\n",
    "\n",
    "#         print(\"Analysis completed successfully.\")\n",
    "\n",
    "# Create a preallocated NumPy array to store results\n",
    "    results = np.empty((len(df), 22), dtype=object)\n",
    "    \n",
    "    # Analyze comments in parallel\n",
    "    with futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "        futures_list = []\n",
    "        for i, row in tqdm(enumerate(df.itertuples()), total=len(df)):\n",
    "            comment = row.comment\n",
    "            post_text = row.text\n",
    "            score = row.score\n",
    "\n",
    "            # Submit job to executor\n",
    "            future = executor.submit(analyze_comment, comment, post_text, score, sia)\n",
    "\n",
    "            futures_list.append(future)\n",
    "\n",
    "        # Wait for all jobs to complete\n",
    "        results_list = []\n",
    "        for future in tqdm(futures.wait(futures_list)[0], total=len(futures_list)):\n",
    "            results_list.append(future.result())\n",
    "        results = np.vstack(results_list)\n",
    "\n",
    "    print(\"Analysis completed successfully.\") \n",
    "\n",
    "#     # Analyze comments in parallel\n",
    "#     with futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "#         for i, row in tqdm(enumerate(df.itertuples()), total=len(df)):\n",
    "#             comment = row.comment\n",
    "#             post_text = row.text\n",
    "#             score = row.score\n",
    "\n",
    "#             # Submit job to executor\n",
    "#             future = executor.submit(analyze_comment, comment, post_text, score)\n",
    "\n",
    "#             # Store result in preallocated NumPy array\n",
    "#             results[i, :] = future.result()\n",
    "#         print(\"Analysis completed successfully.\")\n",
    "    # Analyze comments in parallel\n",
    "    # for row in tqdm(df.itertuples()):\n",
    "    #     comment = row.comment\n",
    "    #     post_text = row.text\n",
    "    #     score = row.score\n",
    "\n",
    "    #     # Submit job to pool\n",
    "    #     job = pool.apply_async(analyze_comment, (comment, post_text, score), callback=results.append)\n",
    "    #     print(\"Analysis completed successfully.\")\n",
    "    # # Close pool and wait for all jobs to complete\n",
    "    # pool.close()\n",
    "    # pool.join()\n",
    "\n",
    "    # Convert results to a pandas DataFrame\n",
    "    result_df = pd.DataFrame(results, columns=['text', 'comment', 'score', \n",
    "                                                'num_words', 'num_unique_words', 'avg_word_length',\n",
    "                                                'num_punctuation', 'num_uppercase_letters', 'num_uppercase_words',\n",
    "                                                'num_stop_words', 'num_sentences', 'numbers',\n",
    "                                                'links', 'hashtags', 'emojis', 'capslock', 'errors', 'Edit', \n",
    "                                                'non_english_words', 'punctuation_freq', 'mentions','toxicity'])\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjmpuKej90lJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23140\\165674498.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 355/440535 [00:00<05:52, 1250.15it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df_analyzed = analyze_comments(train_df, sia)\n",
    "train_df_analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1678895993256,
     "user": {
      "displayName": "Денис Фауч",
      "userId": "17825234106867911063"
     },
     "user_tz": -600
    },
    "id": "SuT-znYo9YLD"
   },
   "outputs": [],
   "source": [
    "tf.identity"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNgV/iu+Osbu/83lr0L3Jef",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
