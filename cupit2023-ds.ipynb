{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:28:19.194590Z","iopub.execute_input":"2023-03-19T19:28:19.195225Z","iopub.status.idle":"2023-03-19T19:28:20.456275Z","shell.execute_reply.started":"2023-03-19T19:28:19.195175Z","shell.execute_reply":"2023-03-19T19:28:20.454214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Импорт данных**","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef parse_json_line(json_line, id_counter):\n    json_data = json.loads(json_line)\n    text = json_data['text']\n    comments = json_data['comments']\n    comments_text = [comment['text'] for comment in comments]\n    comments_score = [comment['score'] for comment in comments]\n    id_value = next(id_counter)\n    return pd.DataFrame({'id': [id_value]*5, 'text': [text]*5, 'comment': comments_text, 'score': comments_score})\n\nwith open('/kaggle/input/vk-ds-cup/ranking_train.jsonl', 'r') as f:\n    train_data = list(f)\n\nwith open('/kaggle/input/vk-ds-cup/ranking_test.jsonl', 'r') as f:\n    test_data = list(f)\n\nid_counter = iter(range(1, len(train_data)+len(test_data)+1))\n\ntrain_df_list = []\nfor line in train_data:\n    train_df_list.append(parse_json_line(line, id_counter))\n\ntrain_df = pd.concat(train_df_list, ignore_index=True)\n\ntest_df_list = []\nfor line in test_data:\n    test_df_list.append(parse_json_line(line, id_counter))\n\ntest_df = pd.concat(test_df_list, ignore_index=True)","metadata":{"executionInfo":{"elapsed":51882,"status":"ok","timestamp":1678895738169,"user":{"displayName":"Денис Фауч","userId":"17825234106867911063"},"user_tz":-600},"id":"_esfIAZ2wqez","outputId":"74bbefd9-d919-4166-bdff-1d9062e55b5d","execution":{"iopub.status.busy":"2023-03-20T02:54:28.246827Z","iopub.execute_input":"2023-03-20T02:54:28.247544Z","iopub.status.idle":"2023-03-20T02:55:40.463845Z","shell.execute_reply.started":"2023-03-20T02:54:28.247492Z","shell.execute_reply":"2023-03-20T02:55:40.462505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если уже есть данные\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_df = pd.read_csv('/kaggle/working/test_for_model.csv')\n# train_df = test_df # чтобы не менять все дальше","metadata":{"execution":{"iopub.status.busy":"2023-03-20T02:29:54.644543Z","iopub.execute_input":"2023-03-20T02:29:54.645016Z","iopub.status.idle":"2023-03-20T02:29:55.345826Z","shell.execute_reply.started":"2023-03-20T02:29:54.644981Z","shell.execute_reply":"2023-03-20T02:29:55.344768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# installation enchant in linux\n# %%capture installation_linux.log # wtf перестала работать в kaggle\n\n!sudo apt-get -y update > update.log\n!sudo apt-get -y install enchant > enchant.log\n!sudo apt-get -y install enchant-2 >> enchant.log\n!sudo apt-get install -y hunspell-en-us >> enchant.log\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:30:38.951337Z","iopub.execute_input":"2023-03-19T19:30:38.951873Z","iopub.status.idle":"2023-03-19T19:30:54.213809Z","shell.execute_reply.started":"2023-03-19T19:30:38.951819Z","shell.execute_reply":"2023-03-19T19:30:54.211838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%capture installation.log\nimport os\ndef install_java():\n  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n  !java -version\n%env LC_ALL=C.UTF-8\n%env LANG=C.UTF-8\ninstall_java()\n!pip install spacy > installation.log 2>&1\n!pip install language_tool_python >> installation.log 2>&1\n!python -m spacy download en_core_web_sm >> installation.log 2>&1\n!pip install transformers >> installation.log 2>&1\n!pip install tqdm >> installation.log 2>&1\n!pip install psutil >> installation.log 2>&1\n!pip install Cython >> installation.log 2>&1\n!pip install tensorflow-hub >> installation.log 2>&1\n!pip install memory_profiler >> installation.log 2>&1\n!pip install pyenchant >> installation.log 2>&1\n!pip install inflect >> installation.log 2>&1\n!pip install spellchecker >> installation.log 2>&1\n!pip install demoji >> installation.log 2>&1\n!pip install jax jaxlib >> installation.log 2>&1\n!pip install qgrid >> installation.log 2>&1","metadata":{"executionInfo":{"elapsed":52718,"status":"ok","timestamp":1678895790876,"user":{"displayName":"Денис Фауч","userId":"17825234106867911063"},"user_tz":-600},"id":"L7Xi7nprxM0e","execution":{"iopub.status.busy":"2023-03-19T22:20:04.080411Z","iopub.execute_input":"2023-03-19T22:20:04.080795Z","iopub.status.idle":"2023-03-19T22:23:41.569875Z","shell.execute_reply.started":"2023-03-19T22:20:04.080760Z","shell.execute_reply":"2023-03-19T22:23:41.568490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import psutil\n# Возвращает количество ядер CPU\nmax_workers = psutil.cpu_count(logical=True)\nprint(max_workers)","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1678895790877,"user":{"displayName":"Денис Фауч","userId":"17825234106867911063"},"user_tz":-600},"id":"ojyihh9-KW1v","outputId":"b3142684-37ea-4f65-b0d5-19d9b578404a","execution":{"iopub.status.busy":"2023-03-19T22:26:05.161053Z","iopub.execute_input":"2023-03-19T22:26:05.161514Z","iopub.status.idle":"2023-03-19T22:26:05.168718Z","shell.execute_reply.started":"2023-03-19T22:26:05.161470Z","shell.execute_reply":"2023-03-19T22:26:05.167337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Импорт библиотек\n# %%capture analyze_comments.log\nimport re\nimport csv\nimport torch\nimport string\nimport html\nimport numpy as np\nimport spacy\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport nltk\nimport multiprocessing as mp\nimport enchant\nimport language_tool_python\nimport concurrent.futures as futures\nimport cProfile\nimport concurrent.futures\nimport demoji\nimport inflect\nimport multiprocessing\nimport dask.dataframe as dd\nimport qgrid\nfrom cython import compile\nfrom functools import partial\nfrom dask.multiprocessing import get\nfrom nltk.tokenize import word_tokenize\nfrom memory_profiler import profile\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nfrom textblob import TextBlob\nfrom dask.diagnostics import ProgressBar\nfrom numba import jit\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom functools import partial\nfrom tqdm import tqdm\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom transformers import AutoModelForSequenceClassification, AutoModel, AutoTokenizer, TextClassificationPipeline\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.stats import rankdata","metadata":{"executionInfo":{"elapsed":28454,"status":"ok","timestamp":1678896098617,"user":{"displayName":"Денис Фауч","userId":"17825234106867911063"},"user_tz":-600},"id":"5F_YHooqJc0H","outputId":"8078cc96-066f-4bf7-a73d-979357c15a56","execution":{"iopub.status.busy":"2023-03-19T22:26:19.697215Z","iopub.execute_input":"2023-03-19T22:26:19.697834Z","iopub.status.idle":"2023-03-19T22:26:24.430592Z","shell.execute_reply.started":"2023-03-19T22:26:19.697790Z","shell.execute_reply":"2023-03-19T22:26:24.429575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Загрузка pre-trained моделей\nstop_words = set(stopwords.words('english'))\ndict_ench = enchant.Dict(\"en_US\")\ntool = language_tool_python.LanguageTool('en-US')\n\n# nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n# use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n# sia = SentimentIntensityAnalyzer()\n# # Инициация токенайзера\n# model_path = \"martin-ha/toxic-comment-model\"\n# tokenizer = AutoTokenizer.from_pretrained(model_path)\n# model = AutoModelForSequenceClassification.from_pretrained(model_path)\n# pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n# tokenizer_d = AutoTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n# model_d = AutoModel.from_pretrained('distilbert-base-multilingual-cased')","metadata":{"execution":{"iopub.status.busy":"2023-03-19T22:26:27.823697Z","iopub.execute_input":"2023-03-19T22:26:27.824109Z","iopub.status.idle":"2023-03-19T22:26:27.955133Z","shell.execute_reply.started":"2023-03-19T22:26:27.824071Z","shell.execute_reply":"2023-03-19T22:26:27.953511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Добыча стандартных фич\ndef extract_links_and_clean_text(text):\n    if not text:\n        return '', '', 0\n    pattern = r'\\b(?:https?://|www\\.|[a-z0-9.\\-]+\\.[a-z]{2,4}/)(?:[^\\s()<>]+|\\([^\\s()<>]+\\))+(?:\\([^\\s()<>]+\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’])'\n    links = re.findall(pattern, text)\n    if not links:\n        return text, '', 0\n    placeholder = 'LINK'\n    for link in links:\n        cleaned_text = text.replace(link, placeholder)\n    links_string = ' '.join(links)\n    num_links = len(links)\n    return cleaned_text, links_string, num_links\n\ndef analyze_comment(comment, post_text, score, id):\n    \n    num_chars_com = len(comment)\n    num_chars_post = len(post_text)\n    \n#     num_links = extract_links_and_clean_text(comment)\n\n    # Find total number of spaces\n    num_spaces = len(re.findall(r\"\\s\", comment))\n\n    # Find number of digits\n    num_digits = sum([1 for char in comment if char.isdigit()])\n    \n    # Find number of words\n    words = comment.split()\n    num_words = len(words)\n    \n    # Find number of unique words\n    unique_words = set(words)\n    num_unique_words = len(unique_words)\n    \n    # Find number of letters and punctuation marks\n    num_letters = sum([len(word) for word in words])\n    num_punctuation = sum([1 for char in comment if char in string.punctuation])\n    \n    # Find number of uppercase letters and words\n    num_uppercase_letters = sum([1 for char in comment if char.isupper()])\n    num_uppercase_words = sum([1 for word in words if word.isupper()])\n    \n    # Find number of stop words\n    num_stop_words = sum([1 for word in comment.lower().split() if word in stop_words])\n    \n    # Find average word length\n    if num_words > 0:\n        avg_word_length = num_letters / num_words\n    else:\n        avg_word_length = 0\n    \n    # Find number of sentences\n    END_OF_SENTENCE_REGEX = r\"[.!?;\\\\n]\"\n    sentences = re.split(END_OF_SENTENCE_REGEX, comment)\n    sentences = list(filter(lambda x: x.strip(), sentences))\n    num_sentences = len(sentences)\n\n    # Find number of emojis\n    num_emoji = len(demoji.findall(comment).keys())\n    \n    return (id,post_text,comment,score,num_chars_com,num_chars_post,num_spaces,\n            num_digits,num_words,num_unique_words,avg_word_length,num_punctuation,\n            num_uppercase_letters,num_uppercase_words,num_stop_words,num_sentences,\n            comment.count(\"#\"),sum(1 for word in comment.split() if word.isupper() and len(word) >= 2),\n            comment.lower().count('Edit'),sum([1 for word in comment.split() if not word.isascii()]), \n            num_punctuation / len(comment),num_emoji)\n\ndef analyze_comments(df, max_workers, chunk_size=1000):\n\n    results = np.empty((len(df), 22), dtype=object)\n    \n    # Распараллеливание процессов\n    with futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n        futures_list = []\n        i = 0\n        for chunk_df in tqdm(np.array_split(df, len(df) // chunk_size + 1), total=len(df) // chunk_size + 1):\n            for row in chunk_df.itertuples():\n                comment = row.comment\n                post_text = row.text\n                score = row.score\n                id = row.id\n                future = executor.submit(analyze_comment, comment, post_text, score, id)\n                futures_list.append(future)\n                i += 1\n                if i % chunk_size == 0:\n                    # Ожидание, пока не завершится \n                    results_list = []\n                    for future in futures.as_completed(futures_list):\n                        try:\n                            result = future.result()\n                            results_list.append(result)\n                        except Exception as e:\n                            print(f\"Error occurred while processing task: {e}\")\n                    results[i-chunk_size:i] = np.vstack(results_list)\n                    futures_list.clear()\n        print(\"First loop successfully.\")\n        if futures_list:\n            results_list = []\n            for future in tqdm(futures.as_completed(futures_list), total=len(futures_list)):\n                try:\n                    result = future.result()\n                    results_list.append(result)\n                except Exception as e:\n                    print(f\"Error occurred while processing task: {e}\")\n            results[i-len(futures_list):i] = np.vstack(results_list)\n\n    print(\"Analysis completed successfully.\") \n\n    result_df = pd.DataFrame(results, columns=['id', 'post_text', 'comment', 'score', 'num_chars_com', 'num_chars_post', 'num_spaces', 'num_digits', \n                                                'num_words', 'num_unique_words', 'avg_word_length',\n                                                'num_punctuation', 'num_uppercase_letters', 'num_uppercase_words',\n                                                'num_stop_words', 'num_sentences', 'num_hashtags',\n                                                'num_capslock', 'num_Edit', 'num_not_ASCII', 'freq_punctuation', 'num_emoji'])\n\n    return result_df","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:31:15.132403Z","iopub.execute_input":"2023-03-19T19:31:15.133045Z","iopub.status.idle":"2023-03-19T19:31:15.176927Z","shell.execute_reply.started":"2023-03-19T19:31:15.132972Z","shell.execute_reply":"2023-03-19T19:31:15.173038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = analyze_comments(train_df, max_workers)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:31:22.109548Z","iopub.execute_input":"2023-03-19T19:31:22.110793Z","iopub.status.idle":"2023-03-19T19:34:21.784475Z","shell.execute_reply.started":"2023-03-19T19:31:22.110722Z","shell.execute_reply":"2023-03-19T19:34:21.783150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Чистка текста\n\ndemoji.download_codes()\np = inflect.engine()\ndictionary = enchant.Dict(\"en_US\")\n\n# def correct_spelling(word, dictionary):\n#     if dictionary.check(word):\n#         return word\n#     else:\n#         suggestions = dictionary.suggest(word)\n#         if len(suggestions) > 0:\n#             return suggestions[0]\n#         else:\n#             return word\n\n#     # Tokenize words\n#     words = word_tokenize(text)\n    \n#     # Remove stop words\n#     words = [w for w in words if not w in stop_words]\n    \n#     # Correct spelling\n#     corrected_words = [correct_spelling(word, dictionary) for word in words]\n    \n#     corrected_words = [\n#         p.number_to_words(word) if word.isdigit() and int(word) <= 100000 else word\n#         for word in corrected_words\n#     ]\n    \n\ndef replace_emojis(text):\n    \n    return demoji.replace_with_desc(text)\n\n\ndef remove_symbols(text):\n    \n    text = re.sub(r'[^\\w\\s.,?!]', '', text)\n    return text\n\n\ndef clean_text(text):\n    \n    text = text.replace('\\n', ' ')\n    cleaned_text, links_string, num_links = extract_links_and_clean_text(text)\n    cleaned_text = demoji.replace_with_desc(cleaned_text)\n    cleaned_text = cleaned_text.encode('ascii', 'ignore').decode()\n    cleaned_text = cleaned_text.strip()\n    return cleaned_text, links_string or None, num_links or None\n\ndef clean_chunk(df, column_name, index_slice):\n    cleaned_chunk = []\n    links_chunk = []\n    num_links_chunk = []\n    for i in index_slice:\n        cleaned_comment, links_string, num_links = clean_text(df.loc[i, column_name])\n        cleaned_chunk.append(cleaned_comment)\n        links_chunk.append(links_string)\n        num_links_chunk.append(num_links if num_links is not None else 0)\n    return cleaned_chunk, links_chunk, num_links_chunk\n\ndef clean_comments(df, column_name, max_workers):\n    cleaned_comments = []\n    links_strings = []\n    num_links = []\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        index_chunks = np.array_split(df.index, 10000)\n        results = [executor.submit(clean_chunk, df, column_name, index_slice) for index_slice in index_chunks]\n        for result in tqdm(concurrent.futures.as_completed(results), total=len(index_chunks)):\n            cleaned_chunk, links_chunk, num_links_chunk = result.result()\n            cleaned_comments.extend(cleaned_chunk)\n            links_strings.extend(links_chunk)\n            num_links.extend(num_links_chunk)\n\n    cleaned_df = pd.DataFrame({\n        'cleaned_' + column_name: cleaned_comments,\n        'links_from_' + column_name: links_strings,\n        'num_links_from_' + column_name: num_links,\n        'index_col': df.index\n    })\n    # Sort by index\n    cleaned_df = cleaned_df.sort_values(by='index_col')\n    df = df.sort_index()\n    cleaned_df = cleaned_df.set_index('index_col')\n    df = df.join(cleaned_df)\n    return df\n\ndef clean_html_entities(text):\n    return html.unescape(text)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:42:30.208925Z","iopub.execute_input":"2023-03-19T19:42:30.209979Z","iopub.status.idle":"2023-03-19T19:42:30.234017Z","shell.execute_reply.started":"2023-03-19T19:42:30.209910Z","shell.execute_reply":"2023-03-19T19:42:30.232432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = clean_comments(train_df, 'comment', max_workers)\ntrain_df = clean_comments(train_df, 'post_text', max_workers)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:42:38.287288Z","iopub.execute_input":"2023-03-19T19:42:38.287911Z","iopub.status.idle":"2023-03-19T19:47:44.922485Z","shell.execute_reply.started":"2023-03-19T19:42:38.287861Z","shell.execute_reply":"2023-03-19T19:47:44.921009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['cleaned_comment'] = train_df['cleaned_comment'].apply(clean_html_entities)\ntrain_df['cleaned_post_text'] = train_df['cleaned_post_text'].apply(clean_html_entities)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:53:15.720436Z","iopub.execute_input":"2023-03-19T19:53:15.721151Z","iopub.status.idle":"2023-03-19T19:53:16.380322Z","shell.execute_reply.started":"2023-03-19T19:53:15.721093Z","shell.execute_reply":"2023-03-19T19:53:16.378744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Проверка на отсутствие пустых ячеек\nprint(\"Проверка в комментариях\")\nprint(\"Всего строк:\", train_df['cleaned_comment'].count())\nprint(\"Из них не пустых:\", train_df['cleaned_comment'].astype(str).apply(lambda x: len(x.strip()) > 0).sum())\nprint(\"\\nПроверка в постах\")\nprint(\"Всего строк:\", train_df['cleaned_post_text'].count())\nprint(\"Из них не пустых:\", train_df['cleaned_post_text'].astype(str).apply(lambda x: len(x.strip()) > 0).sum())","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:53:36.684880Z","iopub.execute_input":"2023-03-19T19:53:36.685389Z","iopub.status.idle":"2023-03-19T19:53:36.829666Z","shell.execute_reply.started":"2023-03-19T19:53:36.685347Z","shell.execute_reply":"2023-03-19T19:53:36.828081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если пустые есть, заменяем их на оригинальные\ndef replace_empty_comment_with_original(df, cleaned_text, original_text):\n    for i, row in tqdm(df.iterrows()):\n        if len(str(row[cleaned_text]).strip()) < 1:\n            df.loc[i, cleaned_text] = row[original_text]\n    return df\n\ntrain_df  = replace_empty_comment_with_original(train_df, 'cleaned_comment', 'comment')\ntrain_df  = replace_empty_comment_with_original(train_df, 'cleaned_post_text', 'post_text')","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:53:24.995095Z","iopub.execute_input":"2023-03-19T19:53:24.995664Z","iopub.status.idle":"2023-03-19T19:53:33.053761Z","shell.execute_reply.started":"2023-03-19T19:53:24.995602Z","shell.execute_reply":"2023-03-19T19:53:33.052257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Добавление дополнительных фич в виде распознанных в тексте объектов из модели en_core_web_sm\ndef add_entity_column(df, column_name, max_workers):\n    nlp = spacy.load(\"en_core_web_sm\")\n    entity_types = [\"PERSON\", \"FAC\", \"ORG\", \"GPE\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"MONEY\"]\n    for entity_type in entity_types:\n        new_column_name = f\"entity_{entity_type}_{column_name}\"\n        df[new_column_name] = 0\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        for i, row in tqdm(df.iterrows(), total=len(df)):\n            text = row[column_name]\n            entity_count = process_row(text, nlp)\n            for entity_type in entity_types:\n                new_column_name = f\"entity_{entity_type}_{column_name}\"\n                df.at[i, new_column_name] = entity_count[entity_type]\n    return df\n\ndef process_row(text, nlp):\n    entity_count = {\"PERSON\": 0, \"FAC\": 0, \"ORG\": 0,\n                    \"GPE\": 0, \"PRODUCT\": 0, \"EVENT\": 0,\n                    \"WORK_OF_ART\": 0, \"LAW\": 0, \"MONEY\": 0}\n    doc = nlp(text)\n    for ent in doc.ents:\n        if ent.label_ in entity_count:\n            entity_count[ent.label_] += 1\n    return entity_count","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:53:42.144326Z","iopub.execute_input":"2023-03-19T19:53:42.144892Z","iopub.status.idle":"2023-03-19T19:53:42.157772Z","shell.execute_reply.started":"2023-03-19T19:53:42.144846Z","shell.execute_reply":"2023-03-19T19:53:42.155997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = add_entity_column(train_df, 'cleaned_post_text', max_workers=max_workers)\ntrain_df = add_entity_column(train_df, 'cleaned_comment', max_workers=max_workers)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:53:45.860603Z","iopub.execute_input":"2023-03-19T19:53:45.861195Z","iopub.status.idle":"2023-03-19T20:34:59.871074Z","shell.execute_reply.started":"2023-03-19T19:53:45.861140Z","shell.execute_reply":"2023-03-19T20:34:59.869706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['main_cleaned_comment']","metadata":{"execution":{"iopub.status.busy":"2023-03-19T17:57:52.420176Z","iopub.status.idle":"2023-03-19T17:57:52.420599Z","shell.execute_reply.started":"2023-03-19T17:57:52.420383Z","shell.execute_reply":"2023-03-19T17:57:52.420405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Проверка на наличие пустых строк\nfiltered_df = train_df[train_df['cleaned_comment'].apply(lambda x: len(x.strip()) < 1)]\nprint(filtered_df['cleaned_comment'])\n","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:35:08.787040Z","iopub.execute_input":"2023-03-19T20:35:08.787595Z","iopub.status.idle":"2023-03-19T20:35:08.854738Z","shell.execute_reply.started":"2023-03-19T20:35:08.787554Z","shell.execute_reply":"2023-03-19T20:35:08.853677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\n# Распределение очищенных комментариев в зависимости от их длины\nfig = px.histogram(train_df, x=train_df['cleaned_comment'].str.len(), nbins=50,\n                   title='Distribution of Comment Lengths')\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:35:12.228128Z","iopub.execute_input":"2023-03-19T20:35:12.228683Z","iopub.status.idle":"2023-03-19T20:35:12.533047Z","shell.execute_reply.started":"2023-03-19T20:35:12.228626Z","shell.execute_reply":"2023-03-19T20:35:12.531498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Выделение значимости предложения согласно TextBlob до 300 символов\nfrom textblob import TextBlob\n\ndef extract_main_gist(comment):\n    # Use TextBlob to extract keywords and noun phrases\n    blob = TextBlob(comment)\n    keywords = [word for word, tag in blob.tags if tag in ['NN', 'NNP', 'NNS', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]\n    noun_phrases = [np for np in blob.noun_phrases]\n    summary = ' '.join(keywords + noun_phrases)[:300] + '...'  # Combine and truncate to 300 characters\n    return summary\n\n# def correct_text(text):\n#     blob = TextBlob(text)\n#     return str(blob.correct())\n\ndef get_sentiment(comment):\n    blob = TextBlob(comment)\n    sentiment = blob.sentiment.polarity\n    return sentiment\n\ntqdm.pandas()\n\ndef process_comments(df, column, max_workers):\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # Распараллеливание выполнения функции extract_main_gist\n        main_comments = list(tqdm(executor.map(extract_main_gist, df[column]), total=len(df)))\n        \n        # Распараллеливание выполнения функции get_sentiment\n        sentiments = list(tqdm(executor.map(get_sentiment, df[column]), total=len(df)))\n    \n    new_column_main = 'main_' + column\n    new_column_sent = 'sentiment_' + column\n    df[new_column_main] = main_comments\n    df[new_column_sent] = sentiments\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:35:24.582720Z","iopub.execute_input":"2023-03-19T20:35:24.583292Z","iopub.status.idle":"2023-03-19T20:35:24.599588Z","shell.execute_reply.started":"2023-03-19T20:35:24.583238Z","shell.execute_reply":"2023-03-19T20:35:24.597864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = process_comments(train_df, 'cleaned_comment', max_workers)\ntrain_df = process_comments(train_df, 'cleaned_post_text', max_workers)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:35:29.011811Z","iopub.execute_input":"2023-03-19T20:35:29.012361Z","iopub.status.idle":"2023-03-19T20:43:50.383176Z","shell.execute_reply.started":"2023-03-19T20:35:29.012317Z","shell.execute_reply":"2023-03-19T20:43:50.381127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# еще раз посмотрим на распределение\nimport plotly.express as px\n\nfig = px.histogram(train_df, x=train_df['main_cleaned_comment'].str.len(), nbins=50,\n                   title='Distribution of Comment Lengths')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:53:56.078895Z","iopub.execute_input":"2023-03-19T20:53:56.083081Z","iopub.status.idle":"2023-03-19T20:53:56.621237Z","shell.execute_reply.started":"2023-03-19T20:53:56.082793Z","shell.execute_reply":"2023-03-19T20:53:56.613114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n\ndef chunk_text(text):\n    max_length = 512\n    chunks = []\n    while len(text) > max_length:\n        chunk = text[:max_length]\n        chunks.append(chunk)\n        text = text[max_length:]\n    chunks.append(text)\n    return chunks\n\n\ndef calculate_similarities(post_text, comment_text):\n    # Initialize the HashingVectorizer\n    vectorizer = HashingVectorizer(n_features=2 ** 10)\n\n    max_similarities = [0.0] * 3\n\n    post_vector = vectorizer.transform([post_text])\n\n    def process_comment(comment_text):\n        # Split the comment text into chunks of up to 512 tokens\n        comment_chunks = chunk_text(comment_text)\n        # Calculate the similarity score for each chunk\n        chunk_scores = []\n        for chunk in comment_chunks:\n            comment_vector = vectorizer.transform([chunk])\n            # Calculate the cosine similarity \n            cosine_sim = cosine_similarity(post_vector, comment_vector)[0][0]\n            # Calculate the Jaccard similarity \n            comment_vector_array = comment_vector.toarray()[0]\n            post_vector_array = post_vector.toarray()[0]\n            intersection = np.logical_and(post_vector_array, comment_vector_array).sum()\n            union = np.logical_or(post_vector_array, comment_vector_array).sum()\n            jaccard_sim = intersection / union if union != 0 else 0\n            # Calculate the Euclidean distance \n            euclidean_dist = euclidean_distances(post_vector, comment_vector)[0][0]\n\n            chunk_scores.append((cosine_sim, jaccard_sim, euclidean_dist))\n\n        comment_similarity = sorted(\n            chunk_scores, key=lambda x: x[0], reverse=True\n        )[:3]\n        return [similarity[0] for similarity in comment_similarity]\n\n    similarities = process_comment(comment_text)\n\n    # Find the maximum similarities\n    for i, similarity in enumerate(similarities):\n        if similarity > max_similarities[i]:\n            max_similarities[i] = similarity\n\n    return max_similarities\n\ndef add_similarities(df):\n    # Add new columns for the max similarity scores\n    similarities = []\n    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n        max_similarities = calculate_similarities(row[\"cleaned_post_text\"], row[\"cleaned_comment\"])\n        similarities.append(max_similarities)\n    df[[\"max_cosine_sim\", \"max_jaccardi_sim\", \"max_euclidean_dist\"]] = pd.DataFrame(similarities)\n    return df\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:54:11.501086Z","iopub.execute_input":"2023-03-19T20:54:11.502053Z","iopub.status.idle":"2023-03-19T20:54:11.525855Z","shell.execute_reply.started":"2023-03-19T20:54:11.501979Z","shell.execute_reply":"2023-03-19T20:54:11.524149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = add_similarities(train_df)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:54:16.188868Z","iopub.execute_input":"2023-03-19T20:54:16.190412Z","iopub.status.idle":"2023-03-19T20:57:33.147074Z","shell.execute_reply.started":"2023-03-19T20:54:16.190325Z","shell.execute_reply":"2023-03-19T20:57:33.145048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Функция для подсчета новых строк очищенных коротких комментариев - только значимые фичи\ndef count_main_features(text):\n    num_chars_com = len(text) # число символов в строке\n    num_punctuation = len([char for char in text if char in string.punctuation]) # число знаков препинания в строке\n    num_capslock = len([word for word in text.split() if word.isupper()]) # число слов в верхнем регистре\n    num_unique_words = len(set(text.split())) # число уникальных слов\n    freq_punctuation = num_punctuation/num_chars_com # частота знаков препинания\n\n    return pd.Series([num_chars_com, num_punctuation, num_capslock, num_unique_words, freq_punctuation])\n\ntrain_df[['num_chars_com_main', 'num_punctuation_main', 'num_capslock_main', 'num_unique_words_main', 'freq_punctuation_main']] = train_df['main_cleaned_comment'].apply(count_main_features)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:57:43.575102Z","iopub.execute_input":"2023-03-19T20:57:43.575687Z","iopub.status.idle":"2023-03-19T20:58:02.213438Z","shell.execute_reply.started":"2023-03-19T20:57:43.575634Z","shell.execute_reply":"2023-03-19T20:58:02.211783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ndef normalize_columns(df):\n    # Определение столбцов с числовыми данными, кроме столбцов id и score\n    numeric_cols = [col for col in df.columns if col not in ['id', 'score'] and df[col].dtype in ['float64', 'int64']]\n    \n    for col in numeric_cols:\n        scaler = StandardScaler()\n        df[col] = scaler.fit_transform(df[[col]])\n\n    return df\n\ntrain_df = normalize_columns(train_df)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:58:08.404963Z","iopub.execute_input":"2023-03-19T20:58:08.405498Z","iopub.status.idle":"2023-03-19T20:58:08.795779Z","shell.execute_reply.started":"2023-03-19T20:58:08.405454Z","shell.execute_reply":"2023-03-19T20:58:08.794256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# посчитаем относительную позицию интересующих показателей по группам id\n\ntrain_df['relative_num_chars'] = train_df.groupby('id')['num_chars_com_main'].transform(lambda x: x.rank(method='dense') / x.shape[0])\n\ntrain_df['relative_freq_punctuation'] = train_df.groupby('id')['freq_punctuation_main'].transform(lambda x: x.rank(method='dense') / x.shape[0])\n\ntrain_df['relative_capslock'] = train_df.groupby('id')['num_capslock'].transform(lambda x: x.rank(method='dense') / x.shape[0])\n\ntrain_df['relative_unique_words'] = train_df.groupby('id')['num_unique_words_main'].transform(lambda x: x.rank(method='dense') / x.shape[0])\n\ntrain_df['relative_emoji'] = train_df.groupby('id')['num_emoji'].transform(lambda x: x.rank(method='dense') / x.shape[0])\n\ntrain_df['relative_cosine_sim'] = train_df.groupby('id')['max_cosine_sim'].transform(lambda x: x.rank(method='dense') / x.shape[0])\n\ntrain_df['relative_links_from_comment'] = train_df.groupby('id')['num_links_from_comment'].transform(lambda x: x.rank(method='dense') / x.shape[0])\n","metadata":{"execution":{"iopub.status.busy":"2023-03-19T21:02:04.723855Z","iopub.execute_input":"2023-03-19T21:02:04.725841Z","iopub.status.idle":"2023-03-19T21:02:48.854408Z","shell.execute_reply.started":"2023-03-19T21:02:04.725754Z","shell.execute_reply":"2023-03-19T21:02:48.852995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv('test_df_norm.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T21:04:15.146328Z","iopub.execute_input":"2023-03-19T21:04:15.147009Z","iopub.status.idle":"2023-03-19T21:04:22.294981Z","shell.execute_reply.started":"2023-03-19T21:04:15.146949Z","shell.execute_reply":"2023-03-19T21:04:22.293722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Анализ данных**","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"darkgrid\")\n\nnumeric_cols = [col for col in train_df.columns if col not in ['id', 'score'] and train_df[col].dtype in ['float64', 'int64']]\nnum_cols_per_row = 3\nnum_rows = int(np.ceil(len(numeric_cols) / num_cols_per_row))\n\nfig, axes = plt.subplots(num_rows, num_cols_per_row, figsize=(15, 5*num_rows))\naxes = axes.flatten()\n\nfor i, col in enumerate(numeric_cols):\n    sns.violinplot(y=col, x=\"score\", data=train_df, palette=\"Pastel1\", ax=axes[i])\n    axes[i].set_title(f\"Violin plot for {col}\")\n\nfor ax in axes[len(numeric_cols):]:\n    ax.remove()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-19T18:24:23.497579Z","iopub.execute_input":"2023-03-19T18:24:23.499407Z","iopub.status.idle":"2023-03-19T18:25:44.910958Z","shell.execute_reply.started":"2023-03-19T18:24:23.499338Z","shell.execute_reply":"2023-03-19T18:25:44.909066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\ndef get_top_abs_correlations(df, cor, top_n=60):\n    # Получить только числовые столбцы\n    numeric_cols = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]\n    \n    # Вычислить корреляции и отбросить избыточные пары\n    corr_df = df.corr(method='pearson').stack().reset_index()\n    corr_df.columns = ['feature_1', 'feature_2', 'corr']\n    corr_df = corr_df[corr_df['feature_1'] < corr_df['feature_2']]\n    \n    # Выбрать топ корреляций по значению\n    corr_df = corr_df[(corr_df['corr'] >= cor) | (corr_df['corr'] <= -cor)]\n    corr_df = corr_df.sort_values(by='corr', ascending=False).head(top_n)\n    \n    # Создать heatmap с выбранными парами и отображением значений корреляции\n    fig = px.imshow(corr_df.pivot('feature_1', 'feature_2', 'corr'),\n                     labels=dict(x='Feature 1', y='Feature 2', color='Correlation'))\n    fig.update_layout(width=800, height=800, font=dict(size=8), \n                      title=f'Top Correlated Features (from: -{cor} to: -1 & from: {cor} to 1)')\n    fig.show()\n\n\n# Использование функции\nget_top_abs_correlations(train_df, 0.7)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T18:38:28.710336Z","iopub.execute_input":"2023-03-19T18:38:28.710956Z","iopub.status.idle":"2023-03-19T18:38:33.074564Z","shell.execute_reply.started":"2023-03-19T18:38:28.710907Z","shell.execute_reply":"2023-03-19T18:38:33.073295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Определим лист наиболее интересных переменных\n\nlist_of_interes = ['id', 'post_text', 'comment', 'score', 'num_chars_com',\n       'num_chars_post', 'num_punctuation', 'num_capslock', 'num_emoji', 'cleaned_comment',\n       'links_from_comment', 'num_links_from_comment', 'main_cleaned_comment','cleaned_post_text',\n       'main_cleaned_post_text', 'max_cosine_sim',\n       'entity_PERSON_cleaned_comment', 'entity_FAC_cleaned_comment', \n       'entity_GPE_cleaned_comment', 'entity_PRODUCT_cleaned_comment',\n       'entity_LAW_cleaned_comment', 'entity_MONEY_cleaned_comment',\n       'num_chars_com_main', 'num_punctuation_main', 'num_capslock_main',\n        'relative_num_chars', 'relative_freq_punctuation']\n\nlist_of_model = ['id', 'score', 'num_chars_com',\n       'num_chars_post', 'num_punctuation', 'num_capslock', 'num_emoji', 'cleaned_comment',\n        'num_links_from_comment', 'cleaned_post_text','max_cosine_sim',\n       'entity_PERSON_cleaned_comment', 'entity_FAC_cleaned_comment', \n       'entity_GPE_cleaned_comment', 'entity_PRODUCT_cleaned_comment',\n       'entity_LAW_cleaned_comment', 'entity_MONEY_cleaned_comment',\n       'num_chars_com_main', 'num_punctuation_main', 'num_capslock_main',\n        'relative_num_chars', 'relative_freq_punctuation']\n\nlist_for_delete_outliers = ['num_emoji', 'num_links_from_comment', 'max_cosine_sim', 'entity_PERSON_cleaned_comment', 'entity_FAC_cleaned_comment', 'entity_LAW_cleaned_comment']","metadata":{"execution":{"iopub.status.busy":"2023-03-19T19:15:29.592722Z","iopub.execute_input":"2023-03-19T19:15:29.593240Z","iopub.status.idle":"2023-03-19T19:15:29.603427Z","shell.execute_reply.started":"2023-03-19T19:15:29.593203Z","shell.execute_reply":"2023-03-19T19:15:29.601707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df[list_of_model]\ntrain_df.to_csv('test_for_model.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T21:05:15.523145Z","iopub.execute_input":"2023-03-19T21:05:15.523743Z","iopub.status.idle":"2023-03-19T21:05:18.046437Z","shell.execute_reply.started":"2023-03-19T21:05:15.523697Z","shell.execute_reply":"2023-03-19T21:05:18.044994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GroupShuffleSplit\nimport random\n# Сплит\nclass DataPreparation:\n    \n    def __init__(self, train_df):\n        self.train_df = train_df\n        self.object_columns = train_df.select_dtypes(include=['object']).columns.tolist()\n        self.ttrain_data = self.train_df.drop(columns=self.object_columns)\n        \n    def prepare_data(self):\n        groups = [self.ttrain_data for _, self.ttrain_data in self.ttrain_data.groupby('id')]\n        import random\n        random.shuffle(groups)\n\n        # мешаем комментарии\n        for i in range(len(groups)):\n            groups[i] =  groups[i].sample(frac=1)\n\n        self.ttrain_data = pd.concat(groups).reset_index(drop=True)\n        gss = GroupShuffleSplit(test_size=.30, n_splits=1, random_state=7).split(self.ttrain_data, groups=self.ttrain_data['id'])\n        X_train_inds, X_test_inds = next(gss)\n\n        traindata = self.ttrain_data.iloc[X_train_inds]\n        X_train = traindata.loc[:, ~traindata.columns.isin(['id','score'])]\n        y_train = traindata.loc[:, traindata.columns.isin(['score'])]\n\n        groups = traindata.groupby('id').size().to_frame('size')['size'].to_numpy()\n\n        testdata = self.ttrain_data.iloc[X_test_inds]\n\n        #We need to keep the id for later predictions\n        X_test = testdata.loc[:, ~testdata.columns.isin(['score'])]\n        y_test = testdata.loc[:, testdata.columns.isin(['score'])]\n\n        return X_train, y_train, X_test, y_test, groups\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-20T00:51:22.779110Z","iopub.execute_input":"2023-03-20T00:51:22.779548Z","iopub.status.idle":"2023-03-20T00:51:22.791117Z","shell.execute_reply.started":"2023-03-20T00:51:22.779508Z","shell.execute_reply":"2023-03-20T00:51:22.789909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_prep = DataPreparation(train_df)\nX_train, y_train, X_test, y_test, groups = data_prep.prepare_data()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T00:51:45.652126Z","iopub.execute_input":"2023-03-20T00:51:45.652576Z","iopub.status.idle":"2023-03-20T00:52:15.028011Z","shell.execute_reply.started":"2023-03-20T00:51:45.652537Z","shell.execute_reply":"2023-03-20T00:52:15.026955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class XGBRankerWrapper:\n    def __init__(self, tree_method='hist', booster='gbtree', objective='rank:ndcg', random_state=42, \n                 learning_rate=0.01, colsample_bytree=1, max_depth=10, n_estimators=110, subsample=0.5):\n        self.model = xgb.XGBRanker(\n            tree_method=tree_method,\n            booster=booster,\n            objective=objective,\n            random_state=random_state,\n            learning_rate=learning_rate,\n            colsample_bytree=colsample_bytree,\n            max_depth=max_depth,\n            n_estimators=n_estimators,\n            subsample=subsample\n        )\n\n    def fit(self, X_train, y_train, groups, verbose=True):\n        self.model.fit(X_train, y_train, group=groups, verbose=verbose)\n    \n    def predict(self, X_test):\n        return self.model.predict(X_test.loc[:, ~X_test.columns.isin(['id'])])\n        \ndef form(line, id_counter):\n    rankings = line[1].rankings\n    id_value = next(id_counter)\n    return pd.DataFrame({'id': [id_value]*5, 'score': rankings})\n\ndef get_result(y):\n    res = []\n    id_counter = iter(y['id'])\n    for line in y.iterrows():\n        res.append(form(line, id_counter))\n    result = pd.concat(res, ignore_index=True)\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-03-20T00:59:32.607685Z","iopub.execute_input":"2023-03-20T00:59:32.608441Z","iopub.status.idle":"2023-03-20T00:59:32.620426Z","shell.execute_reply.started":"2023-03-20T00:59:32.608402Z","shell.execute_reply":"2023-03-20T00:59:32.619412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = XGBRankerWrapper()\ndata_prep = DataPreparation(train_df)\nX_train, y_train, X_test, y_test, groups = data_prep.prepare_data()\nmodel.fit(X_train, y_train, groups)\npredictions = (X_test.groupby('id').apply(lambda x: model.predict(x)))\ny = pd.DataFrame({'id':predictions.index, 'rankings':predictions.values})\nfor i in range(len(y['rankings'])):\n    y['rankings'][i] = y['rankings'][i].argsort().argsort()\nresult = get_result(y)\nresult['true_score'] = y_test['score'].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T00:59:35.384121Z","iopub.execute_input":"2023-03-20T00:59:35.385306Z","iopub.status.idle":"2023-03-20T01:01:23.077497Z","shell.execute_reply.started":"2023-03-20T00:59:35.385259Z","shell.execute_reply":"2023-03-20T01:01:23.076496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Metrics**","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\n\ndef get_score(result):\n    dff = result.groupby(['id'])\n    ids = result.id.unique()\n\n    ndcg = []\n    for i in range(len(ids)):\n        a = ndcg_score(np.asarray([dff.get_group(ids[i])['true_score']]), np.asarray([dff.get_group(ids[i])['score']]))\n        ndcg.append(a)\n\n    fig = px.histogram(ndcg, nbins=50, opacity=0.66)\n    fig.update_layout(title={'text': 'Распределение ndcg-скоров', 'x': 0.5, 'y': 0.8}, \n                      xaxis_title='ndcg-скор', yaxis_title='Количество', bargap=0.1)\n    fig.show()\n\n    scor = sum(ndcg) / len(ndcg)\n    return scor\n\nget_score(result)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T01:14:51.294535Z","iopub.execute_input":"2023-03-20T01:14:51.295468Z","iopub.status.idle":"2023-03-20T01:15:14.682176Z","shell.execute_reply.started":"2023-03-20T01:14:51.295394Z","shell.execute_reply":"2023-03-20T01:15:14.680927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\nclass XGBRankerGridSearch:\n    def __init__(self, params, params_grid):\n        self.params = params\n        self.params_grid = params_grid\n        self.learning_rate = params['learning_rate']\n        self.grow_policy = params['grow_policy']\n\n    def fit(self, X_train, y_train, groups, X_test, y_test):\n        model_pars = []\n        ndcg_score = []\n        for i in tqdm(range(len(self.params_grid['learning_rate']))):\n            self.params.update({'learning_rate': self.params_grid['learning_rate'][i]})\n            for k in tqdm(range(len(self.params_grid['grow_policy']))):\n                self.params.update({'grow_policy': self.params_grid['grow_policy'][k]})\n                print([self.params['learning_rate'], self.params['grow_policy']])\n\n                #model_pars.append([params_grid['learning_rate'][i], params_grid['grow_policy'][k]])\n                model_pars.append([self.params['learning_rate'], self.params['grow_policy']])\n                model = xgb.XGBRanker(tree_method = self.params['tree_method'],\n                                      objective = self.params['objective'],\n                                      colsample_bytree = self.params['colsample_bytree'],\n                                      subsample = self.params['subsample'],\n                                      n_estimators = self.params['n_estimators'],\n                                      random_state = self.params['random_state'], \n                                      learning_rate = self.params['learning_rate'],\n                                      booster = self.params['booster'],\n                                      grow_policy = self.params['grow_policy'],\n                                      max_depth = self.params['max_depth']\n                                     )\n                \n                model.fit(X_train, y_train, group=groups, verbose=True)\n                \n                def predict(model, df):\n                    return model.predict(df.loc[:, ~df.columns.isin(['id'])])\n                \n                predictions = (X_test.groupby('id').apply(lambda x: predict(model, x)))\n\n                y = pd.DataFrame({'id':predictions.index, 'rankings':predictions.values})\n                for i in range(len(y['rankings'])):\n                  y['rankings'][i] = y['rankings'][i].argsort().argsort()\n                \n                def form(line, id_counter):\n                    rankings = line[1].rankings\n                    id_value = next(id_counter)\n                    return pd.DataFrame({'id': [id_value]*5, 'score': rankings})\n\n                def get_result(y):\n                    res = []\n                    id_counter = iter(y['id'])\n                    for line in y.iterrows():\n                        res.append(form(line, id_counter))\n                    result = pd.concat(res, ignore_index=True)\n                    return result\n\n                result = get_result(y)\n                result['true_score'] = y_test['score'].tolist()\n                \n                ndcg_score.append(get_score(result))\n                  \n        print(model_pars)\n        print(ndcg_score)\n        max_score = max(ndcg_score)\n        print(max_score)\n        max_ind = ndcg_score.index(max_score)\n        print(max_ind)\n        self.params.update({'learning_rate': model_pars[max_ind][0]})\n        self.params.update({'grow_policy': model_pars[max_ind][1]})\n","metadata":{"execution":{"iopub.status.busy":"2023-03-20T01:49:42.831150Z","iopub.execute_input":"2023-03-20T01:49:42.831593Z","iopub.status.idle":"2023-03-20T01:49:42.851517Z","shell.execute_reply.started":"2023-03-20T01:49:42.831557Z","shell.execute_reply":"2023-03-20T01:49:42.850402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'tree_method': 'gpu_hist',\n    'objective': 'rank:ndcg',\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'n_estimators': 100,\n    'random_state': 42,\n    'learning_rate': 0.1,\n    'booster': 'gbtree',\n    'grow_policy': 'lossguide',\n    'max_depth': 6\n}\n\nparams_grid = {\n    'learning_rate': [0.1, 0.05, 0.01],\n    'grow_policy': ['depthwise', 'lossguide']\n}\n\nsearch = XGBRankerGridSearch(params, params_grid)\nsearch.fit(X_train, y_train, groups, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T01:25:50.878300Z","iopub.execute_input":"2023-03-20T01:25:50.879289Z","iopub.status.idle":"2023-03-20T01:36:55.646977Z","shell.execute_reply.started":"2023-03-20T01:25:50.879233Z","shell.execute_reply":"2023-03-20T01:36:55.645696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# обновление параметров\nparams.update({'learning rate': model_pars[0]})\nparams.update({'grow_policy': model_pars[1]})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom xgboost import plot_importance\n\ndef my_plot_importance(booster, figsize): \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax, importance_type='cover')\n\nmy_plot_importance(model, (30,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#выбор оптимальных фич\nterf = pd.DataFrame()\nterf['id'] = train_data['id']\nterf['score'] = train_data['score']\nterf['num_chars_com'] = train_data['num_chars_com']\nterf['num_punctuation_main'] = train_data['num_punctuation_main']\nterf['relative_freq_punctuation'] = train_data['relative_freq_punctuation']\n","metadata":{"execution":{"iopub.status.busy":"2023-03-20T02:18:33.354712Z","iopub.execute_input":"2023-03-20T02:18:33.355221Z","iopub.status.idle":"2023-03-20T02:18:33.416089Z","shell.execute_reply.started":"2023-03-20T02:18:33.355182Z","shell.execute_reply":"2023-03-20T02:18:33.414763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-03-20T02:42:21.761262Z","iopub.execute_input":"2023-03-20T02:42:21.761983Z","iopub.status.idle":"2023-03-20T02:44:28.768055Z","shell.execute_reply.started":"2023-03-20T02:42:21.761943Z","shell.execute_reply":"2023-03-20T02:44:28.767011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Модель с новыми параметрами\nmodel = XGBRankerWrapper()\ndata_prep = DataPreparation(terf)\nX_train, y_train, X_test, y_test, groups = data_prep.prepare_data()\nmodel.fit(X_train, y_train, groups)\npredictions = (X_test.groupby('id').apply(lambda x: model.predict(x)))\ny = pd.DataFrame({'id':predictions.index, 'rankings':predictions.values})\nfor i in range(len(y['rankings'])):\n    y['rankings'][i] = y['rankings'][i].argsort().argsort()\nresult = get_result(y)\nresult['true_score'] = y_test['score'].tolist()\nget_score(result)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T02:46:58.300494Z","iopub.execute_input":"2023-03-20T02:46:58.301526Z","iopub.status.idle":"2023-03-20T02:49:21.851165Z","shell.execute_reply.started":"2023-03-20T02:46:58.301479Z","shell.execute_reply":"2023-03-20T02:49:21.850268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.DataFrame()\ntest['id'] = train_df['id']\ntest['score'] = train_df['score']\ntest['num_chars_com'] = train_df['num_chars_com']\ntest['num_punctuation_main'] = train_df['num_punctuation_main']\ntest['relative_freq_punctuation'] = train_df['relative_freq_punctuation']","metadata":{"execution":{"iopub.status.busy":"2023-03-20T02:32:38.343670Z","iopub.execute_input":"2023-03-20T02:32:38.344144Z","iopub.status.idle":"2023-03-20T02:32:38.364658Z","shell.execute_reply.started":"2023-03-20T02:32:38.344100Z","shell.execute_reply":"2023-03-20T02:32:38.363510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2023-03-20T02:44:37.771476Z","iopub.execute_input":"2023-03-20T02:44:37.772100Z","iopub.status.idle":"2023-03-20T02:44:37.788854Z","shell.execute_reply.started":"2023-03-20T02:44:37.772050Z","shell.execute_reply":"2023-03-20T02:44:37.787808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test[['id', 'num_chars_com', 'num_punctuation_main', 'relative_freq_punctuation']]\ny_test = test[['score']]\npredictions = (X_test.groupby('id').apply(lambda x: model.predict(x)))\ny = pd.DataFrame({'id':predictions.index, 'rankings':predictions.values})\nfor i in range(len(y['rankings'])):\n    y['rankings'][i] = y['rankings'][i].argsort().argsort()\nresult = get_result(y)\nresult['true_score'] = y_test['score'].tolist()\nresult","metadata":{"execution":{"iopub.status.busy":"2023-03-20T02:49:32.046978Z","iopub.execute_input":"2023-03-20T02:49:32.047351Z","iopub.status.idle":"2023-03-20T02:50:14.264196Z","shell.execute_reply.started":"2023-03-20T02:49:32.047317Z","shell.execute_reply":"2023-03-20T02:50:14.262969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\na = np.empty((70020))\na[:] = np.nan\nA = copy.deepcopy(test_df.drop(columns=['score']))\nA['score'] = a\nB = copy.deepcopy(result['score'])\nA['score'] = B.tolist()\nids = A.id.unique()\ngrs = A.groupby(['id'])\ngrs.get_group(ids[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-20T02:55:55.327370Z","iopub.execute_input":"2023-03-20T02:55:55.327777Z","iopub.status.idle":"2023-03-20T02:55:55.396226Z","shell.execute_reply.started":"2023-03-20T02:55:55.327742Z","shell.execute_reply":"2023-03-20T02:55:55.395124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_list = []\nfor i in range(len(ids)):\n  line = dict()\n  com_line = []\n  comments = [i for i in grs.get_group(ids[i])[\"comment\"]]\n  scores = [i for i in grs.get_group(ids[i])[\"score\"]]\n  text = grs.get_group(ids[i])[\"text\"].iloc[0]\n  for j in range(5):\n    comm = dict()\n    comm[\"text\"] = comments[j]\n    comm[\"score\"] = scores[j]\n    com_line.append(comm)\n  line[\"text\"] = text\n  line[\"comments\"] = com_line\n  df_list.append(line)\n\nwith open('ranking_test.jsonl', 'w') as f:\n  for j in df_list:\n    f.write(str(json.dumps(j)))\n    f.write('\\n')\n    \nf.close()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T02:56:14.159795Z","iopub.execute_input":"2023-03-20T02:56:14.160346Z","iopub.status.idle":"2023-03-20T02:56:30.131030Z","shell.execute_reply.started":"2023-03-20T02:56:14.160309Z","shell.execute_reply":"2023-03-20T02:56:30.129820Z"},"trusted":true},"execution_count":null,"outputs":[]}]}